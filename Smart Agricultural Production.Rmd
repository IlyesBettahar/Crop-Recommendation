---
title: "Smart_Agricultural Production"
author: "Bettahar Ilyes"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,message = FALSE)
```

### 0. Introduction

The goal of this project is to build a machine learning model that can
predict the type of crop based on various environmental factors, such as
the levels of nitrogen, phosphorus, and potassium in the soil, as well
as temperature and pH. The dataset we will be using is [the Smart
Agricultural Production Optimizing Engine
dataset](https://www.kaggle.com/datasets/78c7595cbf5a98a8cc488e5b9f6911f216680e5a221e6704ba7be4d7ef42c753?resource=download)
from Kaggle, which contains information on the aforementioned factors
for different types of crops. <br> To accomplish this goal, we will be
following a typical machine learning project workflow. This includes
steps such as data exploration, data cleaning and preprocessing, feature
selection and engineering, model selection and evaluation, and
hyperparameter tuning. Specifically, we will be dividing the dataset
into training and testing sets, using the training set to train
different machine learning models, and evaluating their performance on
the testing set.<br> The successful completion of this project can have
a significant impact on the agricultural industry by enabling farmers to
optimize their crop yields based on environmental factors, leading to
increased efficiency and profitability.<br>

### 1. Import And Read Data

```{r}
library(tidyverse)
data_set <- read_csv("Crop_recommendation.csv")
```

### 2. Data description

Here I will outline the definitions of the columns in the titanic
dataset.<br> **N:** Nitrogen<br> **P:** Phosphorous<br> **K:**
Potassium<br> **Temperature:** The average soil temperatures for
bioactivity range from 50 to 75F.<br> **Humidity:** The amount of
Humidity in the air.<br> **Ph:** A scale used to identify acidity or
basicity nature; (Acid Nature- Ph\<7; Neutral- Ph=7; Base
Nature-P\>7)<br> **Rainfall:** The amount of rainfall in mm.<br>
**label:** Types of Crop (Rice,Maize, Chickpea; Kidney beans;
pigeonpeas; mothbeans; mungbean;blackgram; lentil; pomegranate; banana;
mango; grapes; watermelon; muskmelon; apple; orange;papaya; coconut;
cotton; jute; coffee)<br>

### 3. Exploratory Data Analysis

Exploratory Data Analysis (EDA) refers to the method of examining and
interpreting data through visualization and analysis to uncover
insights. Essentially, the aim is to extract significant features and
patterns from the data to obtain a better comprehension of the dataset.

#### 3.1 Data Types

```{r,}
# The structure of data_set and the data types of its variables
str(data_set)
```

\`

#### 3.2. Missing Data

```{r,message=FALSE,echo=TRUE}
# Get number of missing values by column in data_set
colSums(is.na(data_set))
```

There are no missing values in either the train_data or test_data
datasets for any of the variables.

#### 3.3 Summary Statistics

This will give you basic descriptive statistics for each column in the
data frame.

```{r,message=FALSE,echo=TRUE}
# Summary statistics for data_set
summary(data_set)
```

Looking at the summary statistics for the train_data, we can see
that:<br>

The N, P, K, temperature, humidity, ph, and rainfall variables all have
non-negative values, as expected. <br> The minimum value for N is 0,
which could indicate missing data or an unusual data point. We will keep
this in mind when we look at the distributions of the variables.<br> The
ranges for N, P, K, and rainfall are quite large, indicating that there
is a lot of variability in the data.<br> The mean temperature is 25.703
and the mean humidity is 71.49, indicating that the data corresponds to
a warm and moderately humid environment.<br> The mean ph is 6.469, which
is slightly acidic but still within the range that many plants can
tolerate.<br> The most common label in train_data is not immediately
obvious from the summary statistics, as the label variable is stored as
a character string. We will need to look at the frequency table for
label to get a better sense of the class distribution.

### 4. Feature Analysis

In feature analysis, we analyze the characteristics of the features or
variables in the dataset. This step is important as it helps us to
understand which features are important in predicting the target
variable and which features can be discarded.

#### Numerical Variables Correlation With Label

```{r}
library(ggplot2)
library(reshape2)
library(RColorBrewer)

cor_data <- cor(data_set[, c('N', 'P', 'K', 'temperature', 'humidity', 'ph', 'rainfall')], use = "complete.obs")
melt_data <- melt(cor_data)

ggplot(data = melt_data, aes(x = Var1, y = Var2, fill = value)) + 
  geom_tile(color = "white") +
  geom_text(aes(label = sprintf("%.2f", value)), color = "black", size = 4) +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, 
                                    size = 12, hjust = 1))

```

#### 4.1. Numerical Variable: Nitrogen

```{r}
# The distribution of the numerical variable "N"
ggplot(data_set, aes(x = N)) + 
  geom_histogram(binwidth = 1, fill = "lightblue", color = "black") +
  ggtitle("Distribution of Nitrogen") + 
  scale_x_continuous(breaks = seq(0, 140, 5)) +
  scale_y_continuous(breaks = seq(0, 40, 2)) +
  labs(x="Nitrogen", y="Frequency", title="Distribution of Nitrogen") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

```{r}
ggplot(data_set, aes(x = N, y = label)) + 
  geom_boxplot(fill = "lightblue", color = "black") +
  scale_x_continuous(breaks = seq(0, 140, 5)) + 
  labs(x="Nitrogen", y="Label", title="Relation Between Nitrogen and Label")+ 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

#### 4.2. Numerical Variable: Phosphorous

```{r}
# The distribution of the numerical variable "P"
ggplot(data_set, aes(x = P)) + 
  geom_histogram(binwidth = 1, fill = "#FED976", color = "black") +
  ggtitle("Distribution of Nitrogen") + 
  scale_x_continuous(breaks = seq(0, 150, 5)) +
  scale_y_continuous(breaks = seq(0, 50, 2)) +
  labs(x="Phosphorous", y="Frequency", title="Distribution of Phosphorous") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

```{r}
# The distribution of the numerical variable "P"
ggplot(data_set, aes(x = N, y = label)) + 
  geom_boxplot(fill = "#FED976", color = "black") +
  scale_x_continuous(breaks = seq(0, 140, 5)) + 
  labs(x="Phosphorous", y="Label", title="Relation Between Phosphorous and Label")+ 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

#### 4.3. Numerical Variable: Potassium

```{r}
# The distribution of the numerical variable "K"
ggplot(data_set, aes(x = K)) + 
  geom_histogram(binwidth = 1, fill = "#74C476", color = "black") +
  ggtitle("Distribution of Nitrogen") + 
  scale_x_continuous(breaks = seq(0, 205, 5)) +
  scale_y_continuous(breaks = seq(0, 90, 4)) +
  labs(x="Potassium", y="Frequency", title="Distribution of Potassium") + 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1)) 
```

```{r}
# The distribution of the numerical variable "K"
ggplot(data_set, aes(x = K, y = label)) + 
  geom_boxplot(fill = "#74C476", color = "black") +
  scale_x_continuous(breaks = seq(0, 205, 5)) + 
  labs(x="Potassium", y="Label", title="Relation Between Potassium and Label")+ 
  theme_minimal() +
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

#### 4.4. Numerical Variable: Temperature

```{r}
# Temperature Distribution
ggplot(data_set, aes(x=temperature)) + 
  geom_histogram(binwidth=1, color="black", fill="skyblue") + 
  labs(title="Distribution of Temperature", x="Temperature", y="Frequency")
```

```{r}
# Temperature Relation with Label
ggplot(data_set, aes(x=temperature, y=label)) + 
  geom_boxplot(fill="skyblue", color="black") + 
  labs(title="Relation Between Temperature  and Label", x="Temperature", y="Label")+
  scale_x_continuous(breaks = seq(0, 40, 2))
```

#### 4.5. Numerical Variable: Humidity

```{r}
# Humidity distribution
ggplot(data_set, aes(x = humidity)) + 
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  scale_x_continuous(breaks = seq(0, 100, 5)) +
  scale_y_continuous(breaks = seq(0, 100, 5)) +
  ggtitle("Distribution of Humidity") +
  xlab("Humidity(%)") +
  ylab("Label")
```

```{r}
# Humidity relation with label
ggplot(data_set, aes(x = humidity, y = factor(label))) +
  geom_boxplot(fill = "blue", color = "black") +
  scale_x_continuous(breaks = seq(0, 100, 5)) +
  ggtitle("Relation Between Humidity and Label") +
  xlab("Humidity(%)") +
  ylab("Label")
```

#### 4.6. Numerical Variable: ph

```{r}
library(ggplot2)
# Distribution of ph variable
ggplot(data_set, aes(x=ph)) +
  geom_histogram(binwidth = 0.1, color="black", fill="#FCBBA1") +
  scale_x_continuous(breaks = seq(0, 10, 1)) +
  scale_y_continuous(breaks = seq(0, 120, 10))+
  xlab("ph") + ylab("Frequency") +
  ggtitle("Distribution of ph ")
```

```{r}
ggplot(data_set, aes(x=ph, y=label)) +
  geom_boxplot(fill="#FCBBA1", color="black") +
  xlab("pH") + ylab("Label") +
  scale_x_continuous(breaks = seq(0, 10, 1))+
  ggtitle("Relation Between pH and Label")
```

#### 4.5. Numerical Variable: Rainfall

```{r}
ggplot(data_set, aes(x = rainfall)) +
  geom_histogram(fill = "purple", color = "white", binwidth = 10) +
  scale_x_continuous(breaks = seq(0, 300, 10)) +
  scale_y_continuous(breaks = seq(0, 250, 20)) +
  labs(x = "Rainfall", y = "Frequency") +
  ggtitle("Distrubition of Rainfall")+
  theme(axis.text.x = element_text(angle=90, vjust=0.5, hjust=1))
```

#### Relationship Between The Three Variables N, P, K and The Label

```{r}
library(plotly)

plot_ly(data_set, x = ~N, y = ~P, z = ~K, color = ~label, 
        colors = c("#1f77b4", "#ff7f0e"), marker = list(size = 3)) %>%
  layout(scene = list(xaxis = list(title = "N"), 
                      yaxis = list(title = "P"), 
                      zaxis = list(title = "K")))

```

#### Relation Between Label, Temperature and Humidity

```{r}
library(ggplot2)

ggplot(data_set, aes(x = temperature, y = label, color = humidity)) + 
  geom_point() +
  labs(title = "Relation Between Label, Temperature and Humidity", 
       x = "Temperature", 
       y = "Label",
       color = "Humidity") +
  scale_color_gradient(low = "blue", high = "red") 

```

### 5. Modeling

#### 5.1. Feature Selection

```{r}
# Select features and target variable
features <- data_set[, c("N", "P", "K", "temperature", "humidity", "ph", "rainfall")]
target <- data_set[["label"]]

# Split data into training and testing sets
set.seed(2)
train_indices <- sample(nrow(data_set), 0.8 * nrow(data_set))
x_train <- features[train_indices, ]
x_test <- features[-train_indices, ]
y_train <- target[train_indices]
y_test <- target[-train_indices]
```

#### 5.2. K-Nearest Neighbors

```{r}
library(class)
# K-Nearest Neighbors model
knn <- knn(x_train, x_test, y_train, k = 5)

# Predict the target values for the testing set
predicted_values <- knn

# Calculate the accuracy score
accuracy <- sum(predicted_values == y_test) / length(y_test)

# Print the accuracy score
cat("KNN Accuracy is: ", accuracy, "\n")
```

##### 5.2.1 K-Nearest Neighbors: K-Cross Validation

Performing 5-fold cross-validation with KNN algorithm

```{r}
library(caret)
# Create a KNN model with 5 neighbors
knn_model <- train(x = x_train, y = y_train, method = "knn", tuneGrid = data.frame(k = 5),
                   trControl = trainControl(method = "cv", number = 5))
# Print the model
knn_model
# Get the accuracy of the model
accuracy1 <- knn_model$results$Accuracy
mean_accuracy1 <- mean(accuracy1)
cat("\nMean Accuracy:", mean_accuracy1)

```

##### 5.2.2. K-Nearest Neighbors: Hyperparameter Tuning

Performed hyperparameter tuning for KNN algorithm by testing different
values of k and recording the accuracy, kappa and standard deviations of
accuracy and kappa for each value of k.

```{r}
# Load necessary libraries
library(caret)
library(e1071)

# Define the training control
ctrl <- trainControl(method = "cv", number = 5)

# Define the tuning grid
grid <- expand.grid(k = seq(1, 20))

# Train the model using cross-validation and the tuning grid
set.seed(123)
knn_model <- train(label ~ ., data = data_set, method = "knn", trControl = ctrl, tuneGrid = grid)

# Print the best value of K and the corresponding accuracy
print(knn_model$bestTune)
print(knn_model$results)

```

The model is consistent and stable across different folds of
cross-validation.

#### 5.3. Decision Tree

```{r}
library(rpart)
# Create Decision Tree model
dt <- rpart(label ~ ., data = data.frame(x_train, label = y_train), method = "class")

# Predict on test set
y_pred <- predict(dt, x_test, type = "class")

# Calculate accuracy
accuracy <- sum(y_pred == y_test) / length(y_test)
cat("Decision Tree Accuracy is:", accuracy)
```

##### 5.3.1. Decision Tree: K-Cross Validation

```{r}
library(rpart)
library(rpart.plot)
# Create a decision tree model
dt_model <- rpart(label ~ ., data = data_set)
# Perform k-fold cross-validation
set.seed(1)
k <- 5
cv_results <- caret::trainControl(method = "cv", number = k)
cv <- caret::train(label ~ ., data = data_set, method = "rpart", trControl = cv_results)
# View cross-validation results
print(cv$results)

```

##### 5.3.2. Decision Tree: Hyperparameter Tuning

```{r}
library(caret)
# Define the model and its hyperparameter grid
tuned_DT <- train(x = x_train, y = y_train, method = "rpart",
                  tuneGrid = expand.grid(cp = seq(0.01, 0.1, 0.01)),
                  trControl = trainControl(method = "cv", number = 5))

# Print the results
print(tuned_DT)
```

#### 5.4. Random Forest

```{r}
library(randomForest)
# Convert label column to factor
data_set$label <- as.factor(data_set$label)

# Build Random Forest model
RF <- randomForest(label ~ ., data = data_set[train_indices,], ntree = 20, importance = TRUE)

# Make predictions on test set
predicted_values <- predict(RF, newdata = data_set[-train_indices,])
# Evaluate model performance
accuracy <- sum(predicted_values == target[-train_indices])/length(target[-train_indices])
print(paste("Random Forest Accuracy is:", accuracy))
# Print classification report
library(caret)
# Convert target variable to factor with same levels as predicted_values
target_factor <- factor(target[-train_indices], levels = levels(factor(predicted_values)))

# Compute confusion matrix
confusionMatrix(predicted_values, target_factor)

```

##### 5.4.1. Random Forest: K-Cross Validation

```{r}
library(caret)

# Select features and target variable
features <- data_set[, c("N", "P", "K", "temperature", "humidity", "ph", "rainfall")]
target <- data_set[["label"]]

# Convert label column to factor
target <- as.factor(target)

# Define the training control with 5-fold cross-validation
train_control <- trainControl(method = "cv", number = 5)

# Train the Random Forest model with 20 trees
rf_model <- train(features, target, method = "rf", trControl = train_control, ntree = 20)

# Print the model
rf_model

# Get the accuracy of the model
accuracy <- rf_model$results$Accuracy
mean_accuracy <- mean(accuracy)
cat("\nMean Accuracy:", mean_accuracy)

```

##### 5.4.2. Random Forest: Hyperparameter Tuning

```{r}
library(caret)

# Set up the grid of hyperparameters to search
tuned_parameters <- expand.grid(mtry = c(2, 3, 4, 5, 6))

# Train the model with 5-fold cross validation
RF_model <- train(x = x_train, y = y_train, method = "rf", trControl = trainControl(method = "cv", number = 5),
                  tuneGrid = tuned_parameters)

# Print the best model and its hyperparameters
print(RF_model)

# Get the accuracy of the model
accuracy <- RF_model$results$Accuracy
mean_accuracy <- mean(accuracy)
cat("\nMean Accuracy:", mean_accuracy)


```

#### 5.5.  Comparing The Accuracies
```{r}
# Create a data frame to store the accuracies
accuracy_df <- data.frame(Model = c("KNN", "Decision Tree", "Random Forest"),
                           Accuracy = c(0.9795455 , 0.9522727, 0.9954545))

# Plot the accuracies
library(ggplot2)
ggplot(accuracy_df, aes(x = Model, y = Accuracy)) +
  geom_bar(stat = "identity", fill = "blue") +
  geom_text(aes(label = round(Accuracy, 3)), vjust = -0.5) +
  labs(title = "Model Accuracy Comparison",
       x = "Model", y = "Accuracy")

```

### 6. Conclusion 
Based on the three models (KNN, Decision Tree, and Random Forest) that were trained and evaluated, it can be concluded that Random Forest outperformed the other two models with the highest accuracy of 0.9955. This indicates that the Random Forest model is the most suitable model for the given dataset and can be used for predicting the crop labels with a high degree of accuracy.




